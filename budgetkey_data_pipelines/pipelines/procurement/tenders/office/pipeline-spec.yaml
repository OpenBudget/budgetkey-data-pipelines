scraper:
  schedule:
    crontab: 0 0 * * *
  pipeline:
    - run: add_metadata
      parameters:
        name: "procurement-tenders-office"
    # get the main HTML page of the office tenders search
    - run: add_resource
      parameters:
        name: "mr-gov-il-search-office-tenders"
        url: "https://www.mr.gov.il/OfficesTenders/Pages/SearchOfficeTenders.aspx"
        format: "txt"
    # download it (using tzabar runner which does some magic that bypasses network security)
    - run: stream_remote_resources
      runner: tzabar
    # parse all the publisher ids from the main search page
    - run: ..add_publishers_resource
      parameters:
        input_resource: "mr-gov-il-search-office-tenders"
        tender_type: "office"
    # go over each publisher and get urls for all office tenders related to this publisher
    - run: ..add_publisher_urls_resource
      parameters:
        tender_type: "office"
      runner: tzabar
    # adds is_new column which indicates whether item already exists in DB or not (based on publication id)
    - run: ..check_existing
      parameters:
        db-table: procurement_tenders_office
    # download the data from exemption pages which don't exist in DB (is_new == False)
    - run: ..download_pages_data
      runner: tzabar
    # parse the data for each exemption
    - run: ..parse_page_data
      parameters:
        output_resource: office
        tender_type: office
    # Do type conversion if needed
    - run: set_types
    # update the SQL table (based on the primary keys defined in the schemas)
    - run: dump.to_sql
      parameters:
        tables:
          procurement_tenders_office:
            mode: update
            resource-name: office
    - run: dump.to_path
      parameters:
        out-path: /var/datapackages/procurement/tenders/office/scraper

office:
  title: Load office data from DB to flat file

  dependencies:
    - pipeline: ./procurement/tenders/office/scraper

  pipeline:
    - run: add_metadata
      parameters:
        name: "procurement-tenders-office"
    - run: add_sql_resource
      parameters:
        datapackage: /var/datapackages/procurement/tenders/office/scraper/datapackage.json
        resource: office
        table: procurement_tenders_office
    - run: stream_remote_resources
    - run: set_types
    - run: dump.to_path
      parameters:
        out-path: /var/datapackages/procurement/tenders/office/all

processed:
  dependencies:
    - pipeline: ./procurement/tenders/office/office

  pipeline:
    - run: add_metadata
      parameters:
        name: procurement-tenders-office-processed
    # Load scraped office
    - run: load_resource
      parameters:
        url: /var/datapackages/procurement/tenders/office/all/datapackage.json
        resource: office
    # Set types for ES
    - run: set_types
      parameters:
        types:
          end_date:
            es:time-range: to
          start_date:
            es:time-range: from
          documents:
            es:itemType: object
            es:schema:
              fields:
                - {name: link, type: string}
                - {name: description, type: string}
                - {name: update_time, type: string}
    - run: set_primary_key
      parameters:
        office:
          - publication_id
    - run: dump.to_sql
      parameters:
        tables:
          procurement_tenders_office_processed:
            mode: update
            resource-name: office
    - run: dump.to_path
      parameters:
        out-path: /var/datapackages/procurement/tenders/office/processed
